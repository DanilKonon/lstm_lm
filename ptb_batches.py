# -*- coding: utf-8 -*-
"""ptb_batches.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xkOqbZK6ktF_Lc2mVLC5Nd28sSr3UJNI
"""

from pathlib import Path
import pandas as pd
from collections import defaultdict
import copy
import numpy as np
import tensorflow as tf
from tqdm import tqdm
from datetime import datetime

# !tar xvf simple-examples.tgz

test_ptb = Path.cwd() / 'simple-examples/data/' / 'ptb.test.txt'
train_ptb = Path.cwd() / 'simple-examples/data/' / 'ptb.train.txt'
valid_ptb = Path.cwd() / 'simple-examples/data/' / 'ptb.valid.txt'

test_ptb.exists(), train_ptb.exists(), valid_ptb.exists()

def read_ptb_file(ptb_file: Path):
    ptb_dict = defaultdict(int)
    one_line = []
    with ptb_file.open(mode='r') as f:
        for ind, sent in enumerate(f):
            sent_words = sent.strip().split()
            sent_words += ['<eos>']
            for word in sent_words:
                ptb_dict[word] += 1
            one_line += sent_words
    print(ind)
    return ptb_dict, one_line

test_dict, test_ol = read_ptb_file(test_ptb)
train_dict, train_ol = read_ptb_file(train_ptb)
valid_dict, valid_ol = read_ptb_file(valid_ptb)
len(test_ol), len(train_ol), len(valid_ol)

set(train_dict.keys()).issuperset(set(valid_dict.keys()))

word_to_ind = {k: ind for ind, (k, v) in enumerate(train_dict.items())}
ind_to_word = {v: k for k, v in word_to_ind.items()}

num_layers = 2 
emb_size = 200 
batch_size = 20 
num_steps = 35 
vocab_size = len(train_dict)
hidden_size = 200
max_grad_norm = 5

def indecise(one_line: list, word_to_ind: dict) -> list:
    return [word_to_ind[word] for word in one_line]

def batch_to_text(batch, ind_to_word):
    vect_func = np.vectorize(lambda x: ind_to_word[x])
    return vect_func(batch)



def generate_batches(ol, word_to_ind, batch_size=5, num_steps=3):
    ol = ol[0:(len(ol) // (batch_size * num_steps)) * batch_size * num_steps]
    ind_ol = np.array(indecise(ol, word_to_ind))
    ind_ol = ind_ol.reshape([batch_size, -1])
    num_batches = ind_ol.shape[1] // num_steps
    for i in range(num_batches):
        yield ind_ol[:, i * num_steps: (i+1) * num_steps]

gen_batches_X = generate_batches(ol=train_ol, word_to_ind=word_to_ind, batch_size=batch_size, num_steps=num_steps)
gen_batches_Y = generate_batches(ol=train_ol[1:], word_to_ind=word_to_ind, batch_size=batch_size, num_steps=num_steps)

x0 = next(gen_batches_X)
y0 = next(gen_batches_Y)



tf.reset_default_graph()

ph_input_seq = tf.placeholder(dtype=tf.int32, shape=(None, None), name='input_sequence') # bs, seq_len
ph_target_seq = tf.placeholder(dtype=tf.int32, shape=(None, None), name='target_sequence') # bs, seq_len
ph_lr = tf.placeholder(dtype=tf.float32, name='learning_rate')
ph_keep_prob = tf.placeholder(dtype=tf.float32, name='keep_probability')

word_embeddings = tf.get_variable(name='word_embs', 
                                  shape=[vocab_size, emb_size], 
                                  initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))

batch_mat = tf.nn.embedding_lookup(word_embeddings, ph_input_seq, name='word_emb_lookup')  # bs, seq_len, emb_size

# cell_state_1 = tf.placeholder(dtype=tf.float32, shape=(batch_size, hidden_size), name='cell_state_1')
# cell_state_2 = tf.placeholder(dtype=tf.float32, shape=(batch_size, hidden_size), name='cell_state_2')
# hidd_state_1 = tf.placeholder(dtype=tf.float32, shape=(batch_size, hidden_size), name='hidden_state_1')
# hidd_state_2 = tf.placeholder(dtype=tf.float32, shape=(batch_size, hidden_size), name='hidden_state_2')

# lstm_cell_tuple_c1 = tf.contrib.rnn.LSTMStateTuple(
#     cell_state_1,
#     hidd_state_1
# )

# lstm_cell_tuple_c2 = tf.contrib.rnn.LSTMStateTuple(
#     cell_state_2,
#     hidd_state_2
# )

# cell = tf.contrib.rnn.LSTMBlockCell(num_units=hidden_size)

# lstm_layers = [tf.contrib.rnn.LSTMBlockCell(num_units=size) for size in [hidden_size, hidden_size]]
# multi_cell = tf.contrib.rnn.MultiRNNCell(lstm_layers)
# initial_state = multi_cell.zero_state(batch_size, dtype=tf.float32)
# outputs, state = tf.nn.dynamic_rnn(
#                               cell=multi_cell,
#                               inputs=batch_mat,
#                               dtype=tf.float32
#                               initial_state=initial_state)

# cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_size)
# initial_state_cell = cell.zero_state(batch_size, dtype=tf.float32)
# outputs, state = tf.nn.dynamic_rnn(
#                               cell=cell,
#                               inputs=batch_mat,
#                               dtype=tf.float32,
#                               initial_state=initial_state_cell)

lstm_layers = [tf.nn.rnn_cell.LSTMCell(num_units=hidden_size) for _ in range(2)]
multi_cell = tf.contrib.rnn.MultiRNNCell(lstm_layers)
initial_state_multi = multi_cell.zero_state(batch_size, dtype=tf.float32)
outputs, state = tf.nn.dynamic_rnn(
                              cell=multi_cell,
                              inputs=batch_mat,
                              dtype=tf.float32,
                              initial_state=initial_state_multi)

# multi_cell.zero_state(batch_size=batch_size, dtype=tf.float32)

# [[batch_size, s] for s in multi_cell.state_size]

# outputs, state = tf.nn.dynamic_rnn(multi_cell, 
#                                    inputs=batch_mat, 
#                                    dtype=tf.float32, 
#                                    initial_state=(lstm_cell_tuple_c1, lstm_cell_tuple_c2))

logits = tf.contrib.layers.fully_connected(outputs,  # batch_size, num_steps, vocab_size
                                           num_outputs=vocab_size,
                                           activation_fn=None)

probabs = tf.nn.softmax(logits) # batch_size, num_steps, vocab_size

seq_mask = tf.sequence_mask(lengths=[num_steps] * batch_size, maxlen=num_steps, dtype=tf.float32)

losses = tf.contrib.seq2seq.sequence_loss(logits,
                                          ph_target_seq,
                                          weights=seq_mask,
                                          average_across_timesteps=True,
                                          average_across_batch=True)



tvars= tf.trainable_variables()
grads, _ = tf.clip_by_global_norm(tf.gradients(losses, tvars),
                                  max_grad_norm)
optimizer = tf.train.AdamOptimizer(learning_rate=ph_lr, )
train_op = optimizer.apply_gradients(zip(grads, tvars))

tvars

# traininig_op = optimizer.minimize(losses)

saver = tf.train.Saver()

config = tf.ConfigProto()
config.gpu_options.allow_growth = True

summary_loss = tf.summary.scalar(name='scalar_loss', tensor=losses)



num_batches = len(train_ol) // (batch_size) // (num_steps)

def time_str():
    return datetime.now().replace(microsecond=0).isoformat().replace(':', '-')

n_epochs = 30
model_name = 'lm_ptb'
step = 0
lr = 1

with tf.Session(config=config) as sess:
    sess.run(tf.global_variables_initializer())
    train_writer = tf.summary.FileWriter(model_name + '/summaries%s' % time_str(), sess.graph)
    for i in range(n_epochs):
        gen_batches_X = generate_batches(ol=train_ol, word_to_ind=word_to_ind, batch_size=batch_size, num_steps=num_steps)
        gen_batches_Y = generate_batches(ol=train_ol[1:], word_to_ind=word_to_ind, batch_size=batch_size, num_steps=num_steps)
        with tqdm(total=num_batches) as p:
            last_state = None
            for x, y in zip(gen_batches_X, gen_batches_Y):
                if last_state is not None:
                  feed_dict = {initial_state_multi: last_state,
                                 ph_input_seq: x, 
                                 ph_target_seq: y,
                                 ph_lr: lr}
                else:
                  feed_dict = {
                                 ph_input_seq: x, 
                                 ph_target_seq: y,
                                 ph_lr: lr}
                    
                _, last_state = sess.run(
                    [train_op, state],
                    feed_dict=feed_dict)

                if step % 100 == 0:
                    perplex = losses.eval(
                        feed_dict=feed_dict
                    )
                    print(perplex)
                    summ = sess.run(summary_loss, 
                             feed_dict=feed_dict)
                    train_writer.add_summary(summ, global_step=step)
                p.update(1)
                step += 1
        if (i % 1 == 0):
            print('Epoch ', i)
            saver.save(sess,
                       model_name + '/model_nt/model',
                       global_step=i)



sess.close()
sess = tf.InteractiveSession()

tf.reset_default_graph()

tf.trainable_variables()

new_saver = tf.train.import_meta_graph('./lm_ptb/model_nt/model-28.meta')
new_saver.restore(sess, tf.train.latest_checkpoint('./lm_ptb/model_nt'))

gen_batches_X = generate_batches(ol=train_ol, word_to_ind=word_to_ind, batch_size=batch_size, num_steps=num_steps)
gen_batches_Y = generate_batches(ol=train_ol[1:], word_to_ind=word_to_ind, batch_size=batch_size, num_steps=num_steps)
x = next(gen_batches_X)
y = next(gen_batches_Y)
feed_dict = {
    ph_input_seq: x, 
    ph_target_seq: y,
    ph_lr: lr
}

prob = sess.run(probabs, feed_dict=feed_dict)

np.argwhere(prob[0, 1, :] > 0)

0sess.run(tf.global_variables_initializer())

word_embeddings.eval()

stat = sess.run(state, feed_dict={ph_input_seq: x0, ph_target_seq: y0})

stat[1][1].shape

losses[0].shape

sess.run(multi_cell.initial_state)

losses[0].shape

np.array_equal(out[:, 2, :], stat[1])

stat[1].shape

stat[0].shape





import tensorflow as tf
import numpy as np

n_inputs = 3
n_neurons = 5

X_0 = tf.placeholder(dtype=tf.float32, shape=[None, n_inputs])
X_1 = tf.placeholder(dtype=tf.float32, shape=[None, n_inputs])

basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)

output, states = tf.contrib.rnn.static_rnn(basic_cell, [X_0, X_1], dtype=tf.float32)

Y_0, Y_1 = output

X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]]) 
X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]])

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    out, stat = sess.run([output, states], feed_dict={X_0: X0_batch, X_1: X1_batch})

out[0].shape, np.array_equal(out[1], stat)

X0_batch.shape

